{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3257622",
   "metadata": {},
   "source": [
    "# Deep Mixture of Experts for High-Level Intent Regression\n",
    "\n",
    "We want to predict the likely bounds and trajectory of a given UAV during flight, based on its past trajectory and predicted intent class. In an ideal world, we would like to predict the exact path in 3D space over time (4 dimensions). The problem with doing this using a data-driven approach is that it would take a huge amount of data to work effectively, which is currently much beyond the data acquired during this research.\n",
    "\n",
    "Instead, we can simplify the problem to become a regression task, which predicts the likely maximum position of the UAV forward in time for each dimension. This represents a multi-dimensional regression task, which can be achieved using supervised learning through the curation of a dataset using past flights of each intent class.\n",
    "\n",
    "We'll use the same core data that was used for high-level intent classification (four classes: package delivery, perimeter, point-to-point and surveying flights). To create a suitable regression dataset we'll need to form sub-trajectories (like done previously), but also add suitable labels to each sub-trajectory corresponding to the maximum distance travelled from the last time instance in the window to 'n' timesteps in the future (e.g. 1 minute or 30 seconds). This does not need to be for just one set of predictions into the future, but could also include multiple predictions, e.g. one set for 10 seconds, another for 30, and another for 1 minute, which provides a useful set of predictions for where the UAV will be over different time periods in the future.\n",
    "\n",
    "To achieve this, all future rows $n$ timesteps into the future from that sub-trajectory will need to be checked, and the maximum value for each cartesian coordinate used as the output label for each dimension. The type of flight will have a large impact on this, since for example, point-to-point flights will get progressively further and further with time, whereas perimeter or scanning flights might stay peak at a certain value and then decrease over time. \n",
    "\n",
    "The models used in this research are:\n",
    "\n",
    "1. Multi Input Linear Regression (Baseline)\n",
    "\n",
    "2. Multi Input LSTM.\n",
    "\n",
    "3. Multi Input CBLSTM/CBLSTMA.\n",
    "\n",
    "4. Multi Input CNN\n",
    "\n",
    "5. Deep Mixture Of Experts (DMoE). Each expert based on Multi Input CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from Framework.DataGeneration.DataPreprocessing import Preprocessing\n",
    "from Framework.DataGeneration.Standardiser import TrajectoryStandardiser\n",
    "from Framework.TrajectoryRegression.Visualization import Results\n",
    "from Framework.DataGeneration.OutputStandardiser import OutputStandardiser, OutputNormaliser\n",
    "from Framework.TrajectoryRegression.Linear_Regressor import Linear_Regression\n",
    "from Framework.TrajectoryRegression.LSTM_Regressor import MultiInputLSTM\n",
    "from Framework.TrajectoryRegression.CLSTM_Regressor import CLSTM_Regressor\n",
    "from Framework.TrajectoryRegression.CNN_Regressor import CNN_Regressor\n",
    "from Framework.TrajectoryRegression.MoE import MixtureOfExperts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds - ensure consistent results\n",
    "from numpy.random import seed\n",
    "seed(14)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(14)\n",
    "\n",
    "# set consistent style of plots\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a8c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize beautiful plots\n",
    "sns.set_style('white')\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rc('xtick', labelsize = 16) \n",
    "plt.rc('ytick', labelsize = 16)\n",
    "plt.rcParams['font.size'] = 16 \n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.rc('savefig', dpi=300)\n",
    "plt.rc('axes', titlesize = 16, labelsize = 16)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get project path and set data directory for notebook\n",
    "PROJECT_PATH = os.getcwd()\n",
    "\n",
    "DATA_DIR = os.path.join(PROJECT_PATH, 'ResearchData/simulated_measurements')\n",
    "DEST_FOLDER = os.path.join(PROJECT_PATH,'FinalModels/Regression')\n",
    "isExist = os.path.exists(DEST_FOLDER)\n",
    "if not isExist:\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(DEST_FOLDER)\n",
    "    print(\"The new directory is created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1812e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level intent class\n",
    "CLASSES = ['mapping_flight', 'package_delivery', \n",
    "           'perimeter_flight', 'point_point_flight']\n",
    "\n",
    "# final dataframe to store all modelling results\n",
    "final_results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535019f",
   "metadata": {},
   "source": [
    "## Formation of sub-trajectories and lookahead sequences\n",
    "\n",
    "With our dataset of flights we want to process it into suitable sub-trajectories with associated bounding labels $n$ timesteps forward in time. For this, we will define a given window size for our sequences, along with a lookahead size for the considered future timesteps to be used for producing our labels for each sequence. The lookahead period will be measured ahead in time from the last timestep in the sequence. For each sequence we will determine the following from using the lookahead period:\n",
    "\n",
    "$\\bullet$ Maximum and minimum x-coordinate achieved $n$ timesteps in the future.\n",
    "\n",
    "$\\bullet$ Maximum and minimum y-coordinate achieved $n$ timesteps in the future.\n",
    "\n",
    "$\\bullet$ Maximum and minimum z-coordinate achieved $n$ timesteps in the future.\n",
    "\n",
    "From these minimum and maximum figures at $n$, timesteps into the future, the differences between these and the last timestep values of the input sequence will be calculated. These differences will then be used as the values to predict using regression, which represent the possible bounds (in metres) that the UAV might be expected to traverse over the given lookahead interval.\n",
    "\n",
    "We will also be able to define multiple lookahead points (n timesteps). In this research, we predict the bounds as defined above, for time periods of 15 seconds and 30 seconds into the future.\n",
    "\n",
    "This means that for each input sub-trajectory, our models predict as accurately as possibly what the maximum and minimum bounds in 3-dimensional Cartesian space will be at at 15 seconds and 30 seconds.\n",
    "\n",
    "In order to predict these outputs, our models will use the multivariate time-series UAV radar tracked features, along with associated meta data (high-level features and summary features) for each sub-trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e64222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Data preprocessor and plotter\n",
    "Preprocessor = Preprocessing()\n",
    "Plots = Results()\n",
    "\n",
    "# define columns to keep as final input features\n",
    "KEEP_COLS = ['est_x', 'est_y', 'est_z',\n",
    "             'est_vel_x', 'est_vel_y', 'est_vel_z',\n",
    "             'meas_range', 'meas_bearing', 'meas_elevation',\n",
    "             'uav_type', 'uav_intent']\n",
    "\n",
    "# define parameters for regression dataset generation\n",
    "WINDOW_SIZE = 64 # random seed 14 for 8,16 & 32 window sizes\n",
    "\n",
    "if WINDOW_SIZE >= 32:\n",
    "    OVERLAP_FACTOR = 10 # use for window lengths 32 & 64\n",
    "else:\n",
    "    OVERLAP_FACTOR = 5 # use for window lengths of 8 & 16\n",
    "\n",
    "# length of lookahead seq (must be at equal / greater to largest sample time)\n",
    "LOOKAHEAD = 30\n",
    "\n",
    "# sample times (in s) to generate bound labels from lookahead sequence\n",
    "SAMPLE_TIMES = [15, 30]\n",
    "\n",
    "# index for uav type above - for reference purposes during modelling\n",
    "UAV_TYPE_IDX = 9\n",
    "\n",
    "# index for intent class - needed for preprocessing / modelling\n",
    "UAV_INTENT_IDX = 10\n",
    "\n",
    "# define encoding mappers for our categorical features\n",
    "UAV_TYPE_MAP = {'LSS' : 0, \n",
    "                'Fixed-Wing' : 1}\n",
    "\n",
    "# define IDs to use for each intent class\n",
    "UAV_INTENT_MAP = {'mapping_flight' : 0, \n",
    "                  'package_delivery' : 1,\n",
    "                  'perimeter_flight' : 2, \n",
    "                  'point_point_flight' : 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebad2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of subtrajectories based on lookahead sequences\n",
    "X = []\n",
    "y = []\n",
    "y_lookaheads = []\n",
    "flight_refs = []\n",
    "\n",
    "# iterate each class and generate sub-trajectories for flights\n",
    "for class_name in tqdm(CLASSES):\n",
    "    trajs, lookaheads, labels, refs = Preprocessor.get_class_reg_trajectories(class_name, \n",
    "                                                                               DATA_DIR, \n",
    "                                                                               KEEP_COLS, \n",
    "                                                                               WINDOW_SIZE,\n",
    "                                                                               OVERLAP_FACTOR,\n",
    "                                                                               LOOKAHEAD,\n",
    "                                                                               SAMPLE_TIMES)\n",
    "    \n",
    "    X.append(trajs)\n",
    "    y.append(labels)\n",
    "    y_lookaheads.append(lookaheads)\n",
    "    flight_refs.append(refs)\n",
    "\n",
    "# convert overall results into numpy arrays\n",
    "X = np.concatenate(X)\n",
    "y = np.concatenate(y)\n",
    "y_lookaheads = np.concatenate(y_lookaheads)\n",
    "flight_refs = np.concatenate(flight_refs)\n",
    "print(f\"X shape: {X.shape} \\ny shape: {y.shape}\\n\")\n",
    "print(f\"y lookaheads: {y_lookaheads.shape} \\nFlight refs: {flight_refs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276309c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of subtrajectory per class\n",
    "\n",
    "intent_types = X[:, -1:, 0]\n",
    "\n",
    "# assess number trajectories for each label and plot\n",
    "cls_counts = np.unique(intent_types, return_counts=True)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(x=cls_counts[0], height=cls_counts[1], color=['tab:blue', 'tab:green',\n",
    "                                                      'tab:orange', 'tab:red'])\n",
    "plt.title(\"Class counts\", weight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56825773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomalous values (entries with beyond this will be removed)\n",
    "# also remove minimum outliers (use absolute value instead)\n",
    "ANOMALOUS_THRESHOLD = 10000\n",
    "number = y[y.max(axis=1) > ANOMALOUS_THRESHOLD]\n",
    "number.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedbf627",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_idx = y.max(axis=1) > ANOMALOUS_THRESHOLD\n",
    "\n",
    "# remove all anomalous values from our data\n",
    "X = X[~anomalous_idx].copy()\n",
    "y = y[~anomalous_idx].copy()\n",
    "y_lookaheads = y_lookaheads[~anomalous_idx].copy()\n",
    "flight_refs = flight_refs[~anomalous_idx].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce14e8ed",
   "metadata": {},
   "source": [
    "## Training, Validation and Test Splitting\n",
    "\n",
    "We divide our data in suitable training, validation and test splitting sets. Suitable random state is required depending the window size. In this research, we propose some suitable values for the random seed, however some adjustements may be required until a sufficient balanced split of classes is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose appropriate random seed for splits of data\n",
    "if (WINDOW_SIZE == 64) and (LOOKAHEAD == 30):\n",
    "    R_STATE = 12 # use for window size 64\n",
    "elif (WINDOW_SIZE == 64) and (LOOKAHEAD == 40):\n",
    "    R_STATE = 18\n",
    "elif (WINDOW_SIZE == 32) and (LOOKAHEAD == 40):\n",
    "    R_STATE = 13\n",
    "elif ((WINDOW_SIZE == 16) and (LOOKAHEAD == 40 or LOOKAHEAD == 30)):\n",
    "    R_STATE = 13\n",
    "elif (WINDOW_SIZE == 8) and (LOOKAHEAD == 30):\n",
    "    R_STATE = 15  # 18 works good\n",
    "else:\n",
    "    R_STATE = 13\n",
    "R_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation and testing datasets\n",
    "(X_train, y_train, y_train_traj, \n",
    " X_val, y_val, y_val_traj, \n",
    " X_test, y_test, y_test_traj) = Preprocessor.DataSplitter(X, y, y_lookaheads, flight_refs,\n",
    "                                                          val_size = 0.20, test_size = 0.10,\n",
    "                                                          n_splits = 1, random_state = R_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec563adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess number trajectories for each label and plot\n",
    "trg_intents = X_train[:, -1:, 0]\n",
    "cls_counts = np.unique(trg_intents, return_counts=True)\n",
    "\n",
    "ticks = [x for x in range(len(cls_counts[0]))]\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(14,4))\n",
    "ax[0].bar(x=cls_counts[0], height=cls_counts[1], color=['tab:blue', 'tab:green',\n",
    "                                                        'tab:orange', 'tab:red'])\n",
    "ax[0].set_title(\"Class counts (Training)\", weight=\"bold\")\n",
    "ax[0].set_xticks(ticks=ticks, labels=cls_counts[0], \n",
    "                 rotation=45, ha='right')\n",
    "\n",
    "# assess number trajectories for each label and plot\n",
    "val_intents = X_val[:, -1:, 0]\n",
    "cls_counts = np.unique(val_intents, return_counts=True)\n",
    "ax[1].bar(x=cls_counts[0], height=cls_counts[1], color=['tab:blue', 'tab:green',\n",
    "                                                        'tab:orange', 'tab:red'])\n",
    "ax[1].set_title(\"Class counts (Validation)\", weight=\"bold\")\n",
    "ax[1].set_xticks(ticks=ticks, labels=cls_counts[0], \n",
    "                 rotation=45, ha='right')\n",
    "\n",
    "# assess number trajectories for each label and plot\n",
    "test_intents = X_test[:, -1:, 0]\n",
    "cls_counts = np.unique(test_intents, return_counts=True)\n",
    "ax[2].bar(x=cls_counts[0], height=cls_counts[1], color=['tab:blue', 'tab:green',\n",
    "                                                        'tab:orange', 'tab:red'])\n",
    "ax[2].set_title(\"Class counts (Test)\", weight=\"bold\")\n",
    "ax[2].set_xticks(ticks=ticks, labels=cls_counts[0], \n",
    "                 rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25647d8",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Standardisation\n",
    "\n",
    "\n",
    "Just like performed with trajectory classification, we need to standardise our features so that they are scaled appropriately for our modelling. We'll use the means and standard deviations for each feature across all timesteps to standardise each feature to have approximately zero mean and 1 standard deviation. This will only be approximate however, given the time-series nature of the trajectories, which makes standardising effectively more complicated.\n",
    "\n",
    "We also have two categorical columns (uav_type and intent_class), which we need to encode. Since these are constant across the entire trajectory, it makes sense to split these categorical features as seperate features for training. In addition to these high-level categorical features for each sub-trajectory, we will also compute summary point-based features for each sub-trajectory, including means, standard deviations, mins and maxes for each tracked feature. All of these features will be combined into summary (or meta) features.\n",
    "\n",
    "In summary, we have two sets of modelling inputs:\n",
    "\n",
    "1. Sub-Trajectory Multi-Variate Time-series Inputs - These are the UAV radar tracked features over time.\n",
    "\n",
    "2. Summary features - These are the categorical features (UAV type and High-level intent), along with the summarising features for the sub-trajectory (means, std devs, mins, max).\n",
    "\n",
    "For now, the labels will not be standardised, since the predicted responses are conveniently in terms of metres, and in general standardising output labels does not yield significant differences (inputs on the otherhand must be standardized!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56067953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate our scaler and fit_tranform training, transform test\n",
    "standardiser = TrajectoryStandardiser(num_upper_idx=UAV_TYPE_IDX, \n",
    "                        cat_idx=[UAV_TYPE_IDX, UAV_INTENT_IDX],\n",
    "                        cat_mappers=[UAV_TYPE_MAP, UAV_INTENT_MAP])\n",
    "\n",
    "# get standardised sequences and categorical feats\n",
    "X_train_seq_std, X_train_meta = standardiser.fit_transform(X_train)\n",
    "X_val_seq_std, X_val_meta = standardiser.transform(X_val)\n",
    "X_test_seq_std, X_test_meta = standardiser.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be10a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training: \\n - Seqs: {X_train_seq_std.shape}\\n - Meta: {X_train_meta.shape}\")\n",
    "print(f\"\\nValidation: \\n - Seqs: {X_val_seq_std.shape}\\n - Meta: {X_val_meta.shape}\")\n",
    "print(f\"\\nTest: \\n - Seqs: {X_test_seq_std.shape}\\n - Meta: {X_test_meta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ef451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the data has approximately mean zero\n",
    "X_train_seq_std[:, :, :].astype('float').mean(axis=(2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc6149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the data has approximately standard deviation 1\n",
    "X_train_seq_std[:, :, :].astype('float').std(axis=(2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd4fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert final labels into float32 for training\n",
    "y_train = y_train.astype('float32')\n",
    "y_val = y_val.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04412f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only use for standardisation and preprocessing of target values\n",
    "#y_scaler = OutputStandardiser()\n",
    "\n",
    "#y_train_std = y_scaler.fit_transform(y_train)\n",
    "#y_val_std = y_scaler.transform(y_val)\n",
    "#y_test_std = y_scaler.transform(y_test)\n",
    "\n",
    "#y_train_std = y_train_std.astype('float32')\n",
    "#y_val_std = y_val_std.astype('float32')\n",
    "#y_test_std = y_test_std.astype('float32')\n",
    "\n",
    "\n",
    "#y_normer = OutputNormaliser()\n",
    "#y_train_norm = y_normer.fit_transform(y_train)\n",
    "#y_val_norm = y_normer.transform(y_val)\n",
    "\n",
    "#y_original = y_normer.inverse_transform(y_train_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a16d7",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression (Baseline)\n",
    "\n",
    "\n",
    "Similarly to the classification modelling, we'll create some simple baseline classical ML models to benchmark performance and compare against later.\n",
    "\n",
    "Since these models do not support trajectory (sequential) inputs by default, we need to form some appropriate features to use as input for each associated trajectory. We'll do this using means, standard deviations, minimums and maximums.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85fca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary features data to train the Baseline model\n",
    "X_train_sum = standardiser.get_seq_summary_feats(X_train_seq_std)\n",
    "X_val_sum  = standardiser.get_seq_summary_feats(X_val_seq_std)\n",
    "X_test_sum  = standardiser.get_seq_summary_feats(X_test_seq_std)\n",
    "\n",
    "X_train_sum.shape, X_val_sum.shape, X_test_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1740de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Linear Regression Model\n",
    "LR = Linear_Regression()\n",
    "\n",
    "# Traing the linear regression model\n",
    "model_results, val_preds, test_preds = LR.LinearRegressor(X_train_sum, y_train,\n",
    "                                                          X_val_sum, y_val,\n",
    "                                                          X_test_sum, y_test,\n",
    "                                                          WINDOW_SIZE)\n",
    "\n",
    "# Save the results in a Data Frame for future export\n",
    "final_results_df = final_results_df.append(model_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e102484",
   "metadata": {},
   "source": [
    "# Multiple Input LSTM\n",
    "\n",
    "We'll feed both trajectory sequential features and trajectory summary (meta) features into the network. These meta features include the intent label for the flight (one hot encoded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b08b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Multi Input LSTM network\n",
    "lstm_model = MultiInputLSTM(epochs = 75, n_outputs = y.shape[1])\n",
    "lstm_model.MultiInputLSTM(X_train_seq_std, X_train_meta).summary()\n",
    "\n",
    "# Train the Multi Input LSTM network\n",
    "lstm_history = lstm_model.train(X_train_seq_std, X_train_meta, y_train,\n",
    "                 X_val_seq_std, X_val_meta, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE results of the LSTM network\n",
    "Plots.RMSE_plots(lstm_history)\n",
    "\n",
    "# Results under the validation and testing sets\n",
    "model_results, val_preds, test_preds = lstm_model.prediction(X_val_seq_std, X_val_meta, y_val,\n",
    "                                                             X_test_seq_std, X_test_meta, y_test,\n",
    "                                                             WINDOW_SIZE)\n",
    "#append results to our final dataframe of results\n",
    "final_results_df = final_results_df.append(model_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0909b77",
   "metadata": {},
   "source": [
    "# Multi Input CBLSTMA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters of the CLSTM/CBLSTMA network\n",
    "convlstm_params = {'conv_filters':32, 'kernel_size':8,\n",
    "                   'lstm_layers':16, 'cat_dense_units':64, \n",
    "                   'dense_units':64, 'rnn_dropout':0.4, \n",
    "                   'dense_dropout':0.4, 'activation':'relu', \n",
    "                   'lr':1e-3, 'n_outputs':y.shape[1], 'epochs': 125,\n",
    "                  'batch_size': 128}\n",
    "\n",
    "# Instantiate the regression model\n",
    "clstm_model = CLSTM_Regressor(**convlstm_params)\n",
    "clstm_model.MultiInputConvLSTM(X_train_seq_std, X_train_meta).summary()\n",
    "\n",
    "# Train the Multi Input CBLSTMA\n",
    "clstm_history = clstm_model.train(X_train_seq_std, X_train_meta, y_train,\n",
    "                 X_val_seq_std, X_val_meta, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa514c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE results of the CBLSTMA network\n",
    "Plots.RMSE_plots(clstm_history)\n",
    "\n",
    "# Results under the validation and testing sets\n",
    "model_results, val_preds, test_preds, = clstm_model.prediction(X_val_seq_std, X_val_meta, y_val,\n",
    "                                                               X_test_seq_std, X_test_meta, y_test,\n",
    "                                                               WINDOW_SIZE)\n",
    "# append results to our final dataframe of results\n",
    "final_results_df = final_results_df.append(model_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a4103",
   "metadata": {},
   "source": [
    "# Multi Input Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13873413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters of the Multi Input CNN \n",
    "conv_dnn_params = {'conv_filters':32, \n",
    "                   'kernel_size':12, \n",
    "                   'cat_dense_units':64, \n",
    "                   'dense_units':64, \n",
    "                   'dense_dropout':0.4, \n",
    "                   'activation':'relu',\n",
    "                   'loss':'mean_squared_error',\n",
    "                   'lr':5e-4, \n",
    "                   'n_outputs':y.shape[1],\n",
    "                   'epochs': 75,\n",
    "                   'batch_size': 128}\n",
    "\n",
    "# Instantiate the Multi-Input CNN class\n",
    "cnn_model = CNN_Regressor(**conv_dnn_params)\n",
    "cnn_model.MultiInputConv(X_train_seq_std, X_train_meta).summary()\n",
    "\n",
    "# Train the Multi-Input CNN \n",
    "cnn_history = cnn_model.train(X_train_seq_std, X_train_meta, y_train,\n",
    "                 X_val_seq_std, X_val_meta, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE results of the Multi Input CNN\n",
    "Plots.RMSE_plots(cnn_history)\n",
    "\n",
    "# Results under the validation and testing sets\n",
    "model_results, val_preds, test_preds = cnn_model.prediction(X_val_seq_std, X_val_meta, y_val,\n",
    "                                                            X_test_seq_std, X_test_meta, y_test,\n",
    "                                                            WINDOW_SIZE)\n",
    "\n",
    "# append results to our final dataframe of results\n",
    "final_results_df = final_results_df.append(model_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb0fa7",
   "metadata": {},
   "source": [
    "# Deep Mixture of Experts\n",
    "\n",
    "A mixture of experts is a system or framework that consists of a range of expert models that are specialised to work with different kinds or subsets of inputs. The best expert candidates are then chosen by a gating function (or model) for a specific set of inputs. The result is a weighted ensemble of expert models, based on the type of inputs given at that time.\n",
    "\n",
    "This kind of network is perfect for the framework being developed, since the gating function can be the high-level intent classification model previously developed. The predictions from this model can then be used as the gating inputs into the mixture of experts model. There will be a unique expert model trained for each high-level intent class. This will allow each model to focus specifically on the features of each class, which should hopefully help maximise performance.\n",
    "\n",
    "The softmax outputs from the high-level intent classification model for a given sample can represent the weights (or proportions) used to determine the final predictions from the expert models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f508ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the high-level intent classes\n",
    "expert_names =list(UAV_INTENT_MAP.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiInputConvDNN(input_seqs, input_cats, \n",
    "                      conv_filters=20, kernel_size=8,\n",
    "                      cat_dense_units=64, dense_units=64, \n",
    "                      dense_dropout=0.4, activation='relu',\n",
    "                      loss='mean_squared_error',\n",
    "                      lr=1e-3, n_outputs=6):\n",
    "    \"\"\" Convolutional model for trajectory classification.\n",
    "        Uses multiple inputs - both sequences and categorical features.\n",
    "        Uses a Conv layer and several LSTM layers, followed by final \n",
    "        Dense network to classify input sequences. \"\"\"\n",
    "    \n",
    "    seq_input = keras.layers.Input(shape=(input_seqs.shape[1], \n",
    "                                           input_seqs.shape[2]))\n",
    "    \n",
    "    # Conv layers for sequential inputs\n",
    "    cnn_z = keras.layers.Conv1D(filters=conv_filters, kernel_size=kernel_size, \n",
    "                                padding='same', strides=2, \n",
    "                                activation=activation)(seq_input)\n",
    "    \n",
    "    # add dropout regularisation to prevent overfitting\n",
    "    z = keras.layers.Dropout(dense_dropout, name='cnn_dropout')(cnn_z)\n",
    "    \n",
    "    # follow-up convolutional layer\n",
    "    z = keras.layers.Conv1D(filters=conv_filters, kernel_size=kernel_size, \n",
    "                            padding='same', strides=2, \n",
    "                            activation=activation)(z)\n",
    "    \n",
    "    # flatten and apply dense layer\n",
    "    cnn_dense = keras.layers.Flatten()(z)\n",
    "    \n",
    "    # define categorical inputs\n",
    "    cat_input = keras.layers.Input(shape=(input_cats.shape[1]))\n",
    "    \n",
    "    # simple dense layer to process categorical inputs\n",
    "    cat_dense = keras.layers.Dense(cat_dense_units, \n",
    "                activation=activation)(cat_input)\n",
    "    \n",
    "    # combine embeddings from both rnn and categorical components\n",
    "    concat = keras.layers.Concatenate()([cat_dense, cnn_dense])\n",
    "    \n",
    "    # add dropout for regularisation\n",
    "    dense = keras.layers.Dropout(dense_dropout)(concat)\n",
    "    \n",
    "    # dense layer to process combined embeddings\n",
    "    dense = keras.layers.Dense(dense_units, \n",
    "                    activation=activation)(concat)\n",
    "    \n",
    "    # define dense output (no activation) for regression\n",
    "    outputs = keras.layers.Dense(n_outputs)(dense)\n",
    "    \n",
    "    # build overall model with inputs and outputs\n",
    "    model = keras.models.Model(inputs=[seq_input, cat_input], outputs=[outputs])\n",
    "    \n",
    "    # compile model with desired settings\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec7ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters of each expert model\n",
    "model_params = {'input_seqs': X_train_seq_std,\n",
    "                'input_cats': X_train_meta,\n",
    "                'conv_filters':32, \n",
    "                'kernel_size':12,\n",
    "                'cat_dense_units':64, \n",
    "                'dense_units':64, \n",
    "                'activation':'relu',\n",
    "                'loss' : 'huber',\n",
    "                'lr':5e-4, \n",
    "                'n_outputs':y.shape[1]}\n",
    "\n",
    "# lets get our intent labels for training our experts\n",
    "y_trg_intents = np.array(np.argmax(X_train_meta[:, 2:6], axis=1)).flatten()\n",
    "y_val_intents = np.array(np.argmax(X_val_meta[:, 2:6], axis=1)).flatten()\n",
    "y_test_intents = np.array(np.argmax(X_test_meta[:, 2:6], axis=1)).flatten()\n",
    "\n",
    "# Instantiate the Deep Mixture of Experts\n",
    "moe_model = MixtureOfExperts(MultiInputConvDNN, model_params = model_params, \n",
    "                             epochs = 150, expert_names = expert_names)\n",
    "\n",
    "# Compute the training time\n",
    "start_time = time()\n",
    "# Train the DMoE\n",
    "moe_model.train(X_train_seq_std, X_train_meta, y_train, y_trg_intents,\n",
    "                              X_val_seq_std, X_val_meta, y_val, y_val_intents)\n",
    "moe_training_time = time() - start_time\n",
    "print(f\"Deep MoE Total Training Time: {moe_training_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416fae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the RMSE results of each expert\n",
    "Plots.RMSE_DMoE(moe_model)\n",
    "\n",
    "# Plot the RMSE results of training and validation\n",
    "Plots.RMS_pred(moe_model)\n",
    "train_weighted_preds = moe_model.predict_Train(X_train_seq_std, X_train_meta, X_train_meta[:, 2:6])\n",
    "\n",
    "# Prediction time for testing\n",
    "start_pred_time = time()\n",
    "test_weighted_preds = moe_model.predict_Train(X_test_seq_std, X_test_meta, X_test_meta[:, 2:6])\n",
    "test_inference_time = time() - start_pred_time\n",
    "\n",
    "# Results under validation and testing sets\n",
    "(val_weighted_preds, test_weighted_preds) = moe_model.predict(X_val_seq_std, X_val_meta, \n",
    "                                                              X_val_meta[:, 2:6],\n",
    "                                                              X_test_seq_std, X_test_meta,\n",
    "                                                              X_test_meta[:, 2:6])\n",
    "\n",
    "# also work out average time per prediction\n",
    "time_per_pred = test_inference_time / X_test_seq_std.shape[0]\n",
    "print(f\"Deep MoE Average Time per Sample Prediction: {time_per_pred}\")\n",
    "\n",
    "# add our regression metric results to final dataframe\n",
    "model_results = Plots.get_model_results(y_val, val_weighted_preds,\n",
    "                                        y_test, test_weighted_preds, \n",
    "                                        \"Deep MoE\")\n",
    "\n",
    "model_results['Training Time'] = moe_training_time\n",
    "model_results['Avg Pred Time'] = time_per_pred\n",
    "model_results['Window Size'] = WINDOW_SIZE\n",
    "\n",
    "# append results to our final dataframe of results\n",
    "final_results_df = final_results_df.append(model_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74dae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical results of the DMoE\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "trg_rmse = mean_squared_error(y_train, train_weighted_preds, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, val_weighted_preds, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, test_weighted_preds, squared=False)\n",
    "\n",
    "trg_mae = mean_absolute_error(y_train, train_weighted_preds)\n",
    "val_mae = mean_absolute_error(y_val, val_weighted_preds)\n",
    "test_mae = mean_absolute_error(y_test, test_weighted_preds)\n",
    "\n",
    "trg_r2 = r2_score(y_train, train_weighted_preds)\n",
    "val_r2 = r2_score(y_val, val_weighted_preds)\n",
    "test_r2 = r2_score(y_test, test_weighted_preds)\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(f\"    - RMSE: {trg_rmse:.4f}\")\n",
    "print(f\"    - MAE: {trg_mae:.4f}\")\n",
    "print(f\"    - R^2: {trg_r2:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Data:\")\n",
    "print(f\"    - RMSE: {val_rmse:.4f}\")\n",
    "print(f\"    - MAE: {val_mae:.4f}\")\n",
    "print(f\"    - R^2: {val_r2:.4f}\")\n",
    "\n",
    "print(\"\\nTest Data:\")\n",
    "print(f\"    - RMSE: {test_rmse:.4f}\")\n",
    "print(f\"    - MAE: {test_mae:.4f}\")\n",
    "print(f\"    - R^2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our final modelling results to .csv for convenience\n",
    "now = datetime.now()\n",
    "current_date = now.strftime(\"%Y%m%d\")\n",
    "final_results_df.to_excel(f\"{current_date}-final_reg_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff1084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform standardised predictions back to normal (use if outputs were scaled)\n",
    "#trg_preds = y_scaler.inverse_transform(train_weighted_preds)\n",
    "#val_preds = y_scaler.inverse_transform(val_weighted_preds)\n",
    "#test_preds = y_scaler.inverse_transform(test_weighted_preds)\n",
    "\n",
    "trg_preds = train_weighted_preds.copy()\n",
    "val_preds = val_weighted_preds.copy()\n",
    "test_preds = test_weighted_preds.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some random sample trajectories from the X_test sets\n",
    "random_idx = np.random.choice(X_test.shape[0], 10, replace=False)\n",
    "index = 0\n",
    "for idx in random_idx:\n",
    "    print(f\"\\nTest Example Results for idx: {idx}\\n\")\n",
    "    index += 1\n",
    "    # plot 2d results\n",
    "    Plots.plot_bounds_results(idx, index, X_test, y_test, y_test_traj,\n",
    "                              test_preds, UAV_TYPE_IDX, UAV_INTENT_IDX, bound_idx = 0, \n",
    "                              timestep=15, figsize=(10,4), legend = True)\n",
    "    index += 1\n",
    "    Plots.plot_bounds_results(idx, index, X_test, y_test, y_test_traj,\n",
    "                              test_preds, UAV_TYPE_IDX, UAV_INTENT_IDX, bound_idx=1, \n",
    "                              timestep=30, figsize=(10,4), legend= True, anomaly=False)\n",
    "    \n",
    "    #index += 1\n",
    "    # plot 3d results\n",
    "    #Plots.plot_bounds_results_3d(idx, index, X_test, y_test, y_test_traj, \n",
    "    #                             test_preds, UAV_TYPE_IDX, UAV_INTENT_IDX, bound_idx=0, \n",
    "    #                             timestep=15, figsize=(10,10), legend = False)\n",
    "    #index += 1\n",
    "    #Plots.plot_bounds_results_3d(idx, index, X_test, y_test, y_test_traj,\n",
    "    #                             trg_preds, UAV_TYPE_IDX, UAV_INTENT_IDX, bound_idx=1, \n",
    "    #                             timestep=30, figsize=(10,10), legend= False, anomaly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07696d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some random results from the X_val set\n",
    "random_idx = np.random.choice(X_val.shape[0], 10, replace=False)\n",
    "for idx in random_idx:\n",
    "    print(f\"\\nTest Example Results for idx: {idx}\\n\")\n",
    "    index += 1\n",
    "    # plot 2d results\n",
    "    Plots.plot_bounds_results(idx, index, X_val, y_val, y_val_traj,\n",
    "                              val_preds, UAV_TYPE_IDX, UAV_INTENT_IDX, bound_idx = 0, \n",
    "                              timestep=15, figsize=(10,4), legend = True)\n",
    "    index += 1\n",
    "    Plots.plot_bounds_results(idx, index, X_val, y_val, y_val_traj,\n",
    "                              val_preds, UAV_TYPE_IDX, UAV_INTENT_IDX, bound_idx=1, \n",
    "                              timestep=30, figsize=(10,4), legend= True, anomaly=False)\n",
    "    \n",
    "    #index += 1\n",
    "    # plot 3d results\n",
    "    #Plots.plot_bounds_results_3d(idx, index, X_val, y_val, y_val_traj, \n",
    "    #                             val_preds, UAV_TYPE_IDX, UAV_INTENT_IDX, bound_idx=0, \n",
    "    #                             timestep=15, figsize=(10,10), legend = False)\n",
    "    #index += 1\n",
    "    #Plots.plot_bounds_results_3d(idx, index, X_val, y_val, y_val_traj,\n",
    "    #                             val_preds, UAV_TYPE_IDX, UAV_INTENT_IDX, bound_idx=1, \n",
    "    #                             timestep=30, figsize=(10,10), legend= False, anomaly=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
